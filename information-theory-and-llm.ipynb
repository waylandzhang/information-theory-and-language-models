{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **《信息论》** _Claude Shannon's_ **《Information Theory》**\n",
    "\n",
    "1. 信息：定义为减少不确定性的量。\n",
    "\n",
    "2. 一个事件的**信息量**与其发生的**概率**有关。越不可能发生的事件，发生时提供的信息量越大。 $I(x) = -log_{2}(p(x))$\n",
    "\n",
    "3. **熵 Entropy**: 表示平均信息量（或不确定性）。\n",
    "    对于一个离散随机变量 $X$，及其分布概率 $p(x)$ ，熵的公式定义为：$H(X) = - \\sum p(x) log_{2}(p(x))$。熵越高，系统的随机性（或不确定性）越大。\n",
    "\n",
    "\n",
    "现代的电报通讯、ZIP文件压缩、JPEG图像压缩、密码学等都来自于香农信息论这个原理。其主要关注的是数据都统计特性，而不考虑信息的语义或提取其所需的计算成本。比如，\n",
    "\n",
    "\n",
    "- 一段sha256加密的密钥文本\n",
    "- 一段身份证号码\n",
    "\n",
    "后来香农熵又推广到了量子信息领域：量子比特（Qubit）和量子熵等。\n",
    "\n",
    "一个值得注意的点，在香农熵看来两者的具有相同的信息量。但是如果真要提取他们各自的语义信息则所需的计算资源完全不同。\n"
   ],
   "id": "bf282bfc9d69315e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:03:49.062675Z",
     "start_time": "2025-03-26T13:03:49.014185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range for x values (non-negative values up to 1)\n",
    "x = np.linspace(0.001, 1, 100)\n",
    "\n",
    "# Define the function graph\n",
    "y = np.log2(x)\n",
    "\n",
    "# Plot the graph of y vs x\n",
    "plt.plot(x, y, label='log graph')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(f\"{np.log10(0.4)} —— 10为底数的0.4对数值\")\n",
    "print(f\"{np.log(0.4)} —— 自然数e为底数的0.4对数值\")\n",
    "print(f\"{np.log2(0.4)} —— 2为底数的0.4对数值\")"
   ],
   "id": "c807824cbe5a1e6b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3979400086720376 —— 10为底数的0.4对数值\n",
      "-0.916290731874155 —— 自然数e为底数的0.4对数值\n",
      "-1.3219280948873622 —— 2为底数的0.4对数值\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "pe5q31dl3i",
   "source": "上面的对数函数图展示了 $log_2(x)$ 在 $(0, 1]$ 区间的行为。接下来我们用代码直观感受两件事：\n\n1. **二元熵函数**：当一个随机变量只有两种结果时（比如抛硬币），它的熵如何随概率变化？\n2. **信息量的直觉**：不同概率的事件，发生时到底携带多少信息？",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s8dq40eepp",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']\n\n# Binary entropy function H(p) for a two-outcome random variable\np = np.linspace(0.001, 0.999, 500)\nH = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: binary entropy curve\naxes[0].plot(p, H, 'b-', linewidth=2)\naxes[0].set_xlabel('p')\naxes[0].set_ylabel('H(p) bits')\naxes[0].set_title('Binary Entropy Function')\naxes[0].axvline(x=0.5, color='r', linestyle='--', alpha=0.5, label='p=0.5')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Right: information content of different events\nevents = {\n    'Coin flip\\n(heads)': 0.5,\n    'Dice roll\\n(one face)': 1/6,\n    'Common char\\n\"de\"': 0.04,\n    'Rare char\\n\"da\"': 0.0001,\n    'Lottery': 1e-7,\n}\nnames = list(events.keys())\nprobs = list(events.values())\ninfo_content = [-np.log2(p_val) for p_val in probs]\n\ncolors = ['#4CAF50', '#2196F3', '#FF9800', '#F44336', '#9C27B0']\nbars = axes[1].bar(names, info_content, color=colors)\naxes[1].set_ylabel('I(x) bits')\naxes[1].set_title('Information Content of Events')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nfor bar, val, prob in zip(bars, info_content, probs):\n    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n                 f'{val:.1f} bits\\n(p={prob:.4g})', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# The key intuition: rarer events carry more information\nprint(f\"公平硬币正面朝上：I = -log2(0.5)  = {-np.log2(0.5):.1f} 比特\")\nprint(f\"骰子掷出某一面：  I = -log2(1/6)  = {-np.log2(1/6):.2f} 比特\")\nprint(f\"中彩票：          I = -log2(1e-7) = {-np.log2(1e-7):.1f} 比特\")\nprint()\nprint(\"左图可以看到，p=0.5时熵最大——完全不确定的结果包含最多信息。\")\nprint(\"右图很直观：越罕见的事件，发生时给我们的信息量越大。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 互信息\n",
    "\n",
    "对于两个离散随机变量 $X$ 和 $Y$（分别取值 $x_{i}$ 和 $y_{i}$），互信息 $I(X;Y)$ 定义为：\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log_2 \\left( \\frac{p(x, y)}{p(x)p(y)} \\right)\n",
    "$$\n",
    "\n",
    "对于连续随机变量，求和变为积分：\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\iint p(x, y) \\log_2 \\left( \\frac{p(x, y)}{p(x)p(y)} \\right) \\, dx \\, dy\n",
    "$$\n",
    "\n",
    "### 互信息的等价形式\n",
    "\n",
    "互信息可以用熵（Entropy）和条件熵（Conditional Entropy）表达，具有以下几种等价定义：\n",
    "\n",
    "#### 1. 基于熵的定义：\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "\n",
    "- $H(X)$：$X$ 的熵（不确定性）。\n",
    "- $H(Y)$：$Y$ 的熵。\n",
    "- $H(X,Y)$：$X$ 和 $Y$ 的联合熵。\n",
    "- **意义**：互信息是单独熵之和减去联合熵，表示两变量的冗余信息。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 基于条件熵的定义：\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "- $H(X|Y)$：给定 $Y$ 后 $X$ 的条件熵。\n",
    "- **意义**：互信息是 $Y$ 减少的关于 $X$ 的不确定性。\n",
    "\n",
    "同样地：\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "\n",
    "- **非负性**：\n",
    "  - $I(X;Y) \\geq 0$：互信息总是非负的，因为依赖性不会增加不确定性。\n",
    "  - 当 $X$ 和 $Y$ 独立时，$I(X;Y) = 0$。\n",
    "\n",
    "\n",
    "- **上限**：\n",
    "  - $I(X;Y) \\leq H(X)$ 和 $I(X;Y) \\leq H(Y)$：互信息不会超过任一变量的熵。\n",
    "  - 如果 $X = Y$（完全相关），则 $I(X;Y) = H(X) = H(Y)$。\n",
    "\n",
    "\n",
    "- **对称性**：\n",
    "  - $I(X;Y) = I(Y;X)$：互信息不区分变量的先后顺序。\n",
    "\n",
    "\n",
    "- **单位**：\n",
    "  - 以 $\\log_2$ 计算时，单位是比特；\n",
    "  - 用自然对数（$\\ln$）时，单位是奈特（nats）。\n",
    "\n",
    "\n"
   ],
   "id": "cdf77639dfe486ef"
  },
  {
   "cell_type": "markdown",
   "id": "yjh4mic3ieo",
   "source": "上面列出了互信息的数学定义和各种等价形式。下面用一个具体的例子来计算，并画出经典的韦恩图来展示 $H(X)$、$H(Y)$、$H(X,Y)$、$H(X|Y)$、$I(X;Y)$ 之间的关系。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yfcrmmuyun",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Example: Weather(X) vs Umbrella(Y)\n# X: 0=sunny, 1=rainy | Y: 0=no umbrella, 1=umbrella\np_xy = np.array([\n    [0.45, 0.05],  # sunny: no umbrella=0.45, umbrella=0.05\n    [0.05, 0.45],  # rainy: no umbrella=0.05, umbrella=0.45\n])\n\np_x = p_xy.sum(axis=1)\np_y = p_xy.sum(axis=0)\n\nH_X = -np.sum(p_x * np.log2(p_x))\nH_Y = -np.sum(p_y * np.log2(p_y))\nH_XY = -np.sum(p_xy * np.log2(p_xy + 1e-10))\nH_X_given_Y = H_XY - H_Y\nH_Y_given_X = H_XY - H_X\nI_XY = H_X + H_Y - H_XY\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: joint distribution heatmap\nim = axes[0].imshow(p_xy, cmap='Blues', vmin=0, vmax=0.5)\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels(['No umbrella (Y=0)', 'Umbrella (Y=1)'], fontsize=11)\naxes[0].set_yticks([0, 1])\naxes[0].set_yticklabels(['Sunny (X=0)', 'Rainy (X=1)'], fontsize=11)\naxes[0].set_title('Joint Distribution P(X, Y)')\nfor i in range(2):\n    for j in range(2):\n        axes[0].text(j, i, f'{p_xy[i,j]:.2f}', ha='center', va='center',\n                     fontsize=18, fontweight='bold')\nplt.colorbar(im, ax=axes[0], shrink=0.8)\n\n# Right: Venn diagram of entropy relationships\nax = axes[1]\nax.set_xlim(-2, 3.5)\nax.set_ylim(-1.8, 1.8)\nax.set_aspect('equal')\nax.axis('off')\nax.set_title('Entropy Venn Diagram')\n\ncircle_x = plt.Circle((0.5, 0), 1.2, fill=False, edgecolor='#2196F3', linewidth=2.5)\ncircle_y = plt.Circle((1.5, 0), 1.2, fill=False, edgecolor='#F44336', linewidth=2.5)\nax.add_patch(circle_x)\nax.add_patch(circle_y)\n\nax.text(-0.3, 0, f'H(X|Y)\\n{H_X_given_Y:.3f}', ha='center', va='center',\n        fontsize=11, color='#1565C0', fontweight='bold')\nax.text(1.0, 0, f'I(X;Y)\\n{I_XY:.3f}', ha='center', va='center',\n        fontsize=12, color='#6A1B9A', fontweight='bold',\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='#E1BEE7', alpha=0.7))\nax.text(2.3, 0, f'H(Y|X)\\n{H_Y_given_X:.3f}', ha='center', va='center',\n        fontsize=11, color='#C62828', fontweight='bold')\nax.text(0.5, -1.6, f'H(X) = {H_X:.3f}', ha='center', fontsize=11, color='#2196F3')\nax.text(1.5, -1.6, f'H(Y) = {H_Y:.3f}', ha='center', fontsize=11, color='#F44336')\nax.text(1.0, 1.6, f'H(X,Y) = {H_XY:.3f}', ha='center', fontsize=11, color='#333')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"这个例子里，X是天气（晴/雨），Y是是否带伞。\")\nprint(f\"天气的不确定性 H(X) = {H_X:.4f} 比特\")\nprint(f\"带伞的不确定性 H(Y) = {H_Y:.4f} 比特\")\nprint(f\"联合不确定性   H(X,Y) = {H_XY:.4f} 比特\")\nprint(f\"互信息         I(X;Y) = {I_XY:.4f} 比特\")\nprint()\nprint(f\"如果我已经知道某人带了伞，天气的不确定性就从 {H_X:.3f} 降到了 {H_X_given_Y:.3f} 比特。\")\nprint(\"这就是互信息的直觉——知道一个变量后，另一个变量的不确定性减少了多少。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 机器学习中的应用：\n",
    "\n",
    "- 特征选择：选择与目标变量互信息最大的特征。\n",
    "- 信息瓶颈：优化模型压缩输入信息，同时保留与输出相关的互信息。\n",
    "- 语言建模：衡量词语之间的语义依赖。\n",
    "\n",
    "\n",
    "1. **Tokens之间的关系**：\n",
    "\n",
    "  - 在Transformer中，输入是一序列tokens（例如单词或子词），通过自注意力机制（Self-Attention），每个token会根据上下文中的其他token更新其表示。注意力分数（Attention Scores）反映了token之间的重要性和依赖性。\n",
    "\n",
    "  - 从信息论的角度看，两个 token 之间的互信息 $I(X_i; X_j)$ 可以理解为：观察 $X_j$（某个 token 的表示）能减少多少关于 $X_i$（另一个 token 的表示）的不确定性。\n",
    "\n",
    "  - 例如，在句子“猫坐在垫子上”中，“猫”和“坐在”通过注意力机制建立关联。如果“坐在”提供了关于“猫”的上下文信息（比如主语-动词关系），它们的互信息就大于零。\n",
    "\n",
    "2. **注意力如何体现互信息**：\n",
    "\n",
    "  - 自注意力计算每个 token 对其他 token 的加权贡献，权重（softmax 后的注意力分数）可以看作一种条件概率近似：$P(X_i \\mid X_j)$。互信息的计算涉及联合分布 $p(X_i, X_j)$ 和边缘分布 $p(X_i)p(X_j)$ 的比值，而注意力机制隐式地捕捉了这种依赖性。\n",
    "\n",
    "  - 具体来说，注意力机制通过键（Key）、查询（Query）和值（Value）的点积，量化了 token 之间的相关性，这与互信息衡量变量间共享信息的思想有相似之处。\n",
    "\n",
    "3. **采样与建模**：\n",
    "\n",
    "  - 在训练或推理时，采样的 tokens 序列通过多层注意力机制逐步提炼表示。每一层的输出表示可以看作是对输入 tokens 之间互信息的增强提取。例如，经过多层 Transformer 后，“猫”和“垫子”的表示可能包含更多关于它们语义关系的共享信息。\n",
    "\n",
    "\n",
    "_Transformer中的注意力机制并不是直接计算互信息，而是通过参数化（权重矩阵）和训练目标（比如语言建模的交叉熵损失）间接捕捉依赖性。因此，它是互信息的一种近似或实践实现，而不是严格的数学定义。_\n",
    "\n",
    "- Transformer的训练目标通常是最小化交叉熵损失，实际上等价于最小化预测分布与真实分布之间的KL散度（Kullback-Leibler Divergence）\n",
    "- $H(Y \\mid X)$（条件熵）是语言模型预测下一个 token 时的不确定性，互信息 $I(X;Y)$ 则反映了上下文 $X$ 对预测 $Y$ 的帮助。\n",
    "- 基于信息论设计更高效的生成算法，例如通过最大化 $I(X_{t-1}; X_t)$ 优化生成连贯性。\n",
    "\n",
    "- **知识蒸馏（Distillation）**：\n",
    "\n",
    "  互信息可以衡量教师模型和学生模型之间共享的知识量。蒸馏过程试图保留 $I(X;Y)$ 的关键部分。\n",
    "\n",
    "\n",
    "- **LoRA 与微调**：\n",
    "\n",
    "   LoRA（Low-Rank Adaptation）通过低秩更新调整权重，可能影响表示中的互信息分布。信息论可以帮助解释为什么 LoRA 比全参数微调更高效（例如，保留任务相关的互信息）。"
   ],
   "id": "151bf880664b478"
  },
  {
   "cell_type": "markdown",
   "id": "104i8ent2mj",
   "source": "交叉熵损失（Cross-Entropy Loss）是LLM训练的核心目标函数。它和KL散度之间有一个很简洁的关系：\n\n$$H(P, Q) = H(P) + D_{KL}(P \\| Q)$$\n\n也就是说，交叉熵 = 真实分布的熵 + KL散度。模型训练最小化交叉熵，其实就是在最小化KL散度——让模型的预测分布尽可能接近真实分布。下面用一个汉字频率的例子来验证这个关系。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "027d6z6vsglt",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated Chinese character frequency distribution\nchars = ['de', 'shi', 'zai', 'le', 'bu', 'he', 'you', 'ren', 'zhe', 'other']\np_true = np.array([0.040, 0.025, 0.020, 0.018, 0.015, 0.012, 0.011, 0.010, 0.009, 0.840])\n\n# Three \"model\" predictions with different quality\nq_good    = np.array([0.038, 0.023, 0.019, 0.017, 0.014, 0.013, 0.012, 0.011, 0.010, 0.843])\nq_bad     = np.array([0.010, 0.010, 0.010, 0.050, 0.050, 0.050, 0.050, 0.050, 0.050, 0.670])\nq_uniform = np.ones(len(chars)) / len(chars)\n\n# Sanity check: all distributions must sum to 1\nfor name, dist in [('p_true', p_true), ('q_good', q_good), ('q_bad', q_bad)]:\n    assert abs(dist.sum() - 1.0) < 1e-9, f\"{name} sums to {dist.sum()}, not 1.0\"\n\ndef cross_entropy(p, q):\n    return -np.sum(p * np.log2(q + 1e-10))\n\ndef kl_divergence(p, q):\n    return np.sum(p * np.log2((p + 1e-10) / (q + 1e-10)))\n\ndef entropy(p):\n    return -np.sum(p * np.log2(p + 1e-10))\n\nH_p = entropy(p_true)\ndistributions = {'Good model': q_good, 'Bad model': q_bad, 'Uniform': q_uniform}\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: compare distributions (show top 9 characters)\nx_pos = np.arange(len(chars) - 1)\nwidth = 0.2\naxes[0].bar(x_pos - 1.5*width, p_true[:-1], width, label='True P', color='#333', alpha=0.8)\naxes[0].bar(x_pos - 0.5*width, q_good[:-1], width, label='Good model', color='#4CAF50', alpha=0.7)\naxes[0].bar(x_pos + 0.5*width, q_bad[:-1], width, label='Bad model', color='#FF9800', alpha=0.7)\naxes[0].bar(x_pos + 1.5*width, q_uniform[:-1], width, label='Uniform', color='#F44336', alpha=0.7)\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(chars[:-1], fontsize=11)\naxes[0].set_ylabel('Probability')\naxes[0].set_title('True vs Predicted Distributions')\naxes[0].legend(fontsize=10)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Right: cross-entropy and KL divergence comparison\nlabels = list(distributions.keys())\nce_vals = [cross_entropy(p_true, q) for q in distributions.values()]\nkl_vals = [kl_divergence(p_true, q) for q in distributions.values()]\n\nx_pos2 = np.arange(len(labels))\nbars1 = axes[1].bar(x_pos2 - 0.2, ce_vals, 0.35, label='Cross-Entropy H(P,Q)', color='#2196F3', alpha=0.8)\nbars2 = axes[1].bar(x_pos2 + 0.2, kl_vals, 0.35, label='KL Divergence', color='#F44336', alpha=0.8)\naxes[1].axhline(y=H_p, color='green', linestyle='--', linewidth=2, label=f'True Entropy H(P)={H_p:.3f}')\naxes[1].set_xticks(x_pos2)\naxes[1].set_xticklabels(labels, fontsize=11)\naxes[1].set_ylabel('Bits')\naxes[1].set_title('H(P,Q) = H(P) + D_KL(P||Q)')\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3, axis='y')\n\nfor bar, val in zip(bars1, ce_vals):\n    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n                 f'{val:.3f}', ha='center', fontsize=9)\nfor bar, val in zip(bars2, kl_vals):\n    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n                 f'{val:.3f}', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"真实分布的熵 H(P) = {H_p:.4f} 比特\\n\")\nfor name, q in distributions.items():\n    ce = cross_entropy(p_true, q)\n    kl = kl_divergence(p_true, q)\n    print(f\"  {name}: 交叉熵 = {ce:.4f}, KL散度 = {kl:.4f}\")\n    print(f\"    验证: H(P) + D_KL = {H_p:.4f} + {kl:.4f} = {H_p + kl:.4f}\")\nprint()\nprint(\"可以看到，模型预测越接近真实分布，KL散度越小，交叉熵也就越低。\")\nprint(\"这就是为什么LLM训练时用交叉熵作为损失函数——它直接反映了模型与真实语言之间的差距。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n7dv682i7r",
   "source": "上面提到自注意力机制通过 $Q \\cdot K^T$ 点积量化token之间的依赖关系，这和互信息衡量变量间共享信息的思想很像。下面我们把这个过程可视化——用\"猫坐在垫子上\"这个句子，看看注意力矩阵长什么样，再从中近似估算token之间的互信息。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eoj34py0sbe",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Try loading a pretrained Chinese model; fall back to simulated data if unavailable\nuse_model = False\ntry:\n    from transformers import AutoTokenizer, AutoModel\n    import torch\n    model_name = \"bert-base-chinese\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name, output_attentions=True)\n\n    text = \"猫坐在垫子上\"\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Take the last layer's attention, averaged across all heads\n    attention = outputs.attentions[-1].squeeze(0).mean(dim=0).numpy()\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n    use_model = True\n    print(\"已加载 bert-base-chinese 模型\\n\")\nexcept Exception:\n    print(\"未安装 transformers/torch，使用模拟数据演示\")\n    print(\"如需真实模型数据：pip install transformers torch\\n\")\n\n    tokens = ['[CLS]', '猫', '坐', '在', '垫', '子', '上', '[SEP]']\n    # Simulated attention pattern based on linguistic intuition\n    attention = np.array([\n        [0.30, 0.15, 0.10, 0.08, 0.12, 0.10, 0.08, 0.07],  # CLS -> global\n        [0.05, 0.20, 0.35, 0.10, 0.10, 0.08, 0.07, 0.05],  # 猫 -> 坐 (subject-verb)\n        [0.05, 0.30, 0.15, 0.25, 0.10, 0.05, 0.05, 0.05],  # 坐 -> 猫, 在\n        [0.05, 0.08, 0.10, 0.15, 0.25, 0.22, 0.10, 0.05],  # 在 -> 垫子\n        [0.05, 0.05, 0.05, 0.20, 0.20, 0.30, 0.10, 0.05],  # 垫 -> 子, 在\n        [0.05, 0.05, 0.05, 0.10, 0.35, 0.20, 0.15, 0.05],  # 子 -> 垫\n        [0.05, 0.05, 0.15, 0.25, 0.15, 0.15, 0.15, 0.05],  # 上 -> 在, 垫子\n        [0.15, 0.15, 0.12, 0.12, 0.12, 0.12, 0.12, 0.10],  # SEP -> global\n    ])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n\n# Left: attention heatmap\nim = axes[0].imshow(attention, cmap='Reds', vmin=0)\naxes[0].set_xticks(range(len(tokens)))\naxes[0].set_yticks(range(len(tokens)))\naxes[0].set_xticklabels(tokens, fontsize=12)\naxes[0].set_yticklabels(tokens, fontsize=12)\naxes[0].set_xlabel('Key')\naxes[0].set_ylabel('Query')\naxes[0].set_title('Attention Matrix')\nplt.colorbar(im, ax=axes[0], shrink=0.8)\n\n# Right: approximate mutual information from attention weights\n# Strip [CLS] and [SEP], focus on content tokens\ncontent_tokens = tokens[1:-1]\ncontent_attn = attention[1:-1, 1:-1]\ncontent_attn_norm = content_attn / content_attn.sum(axis=1, keepdims=True)\n\nn = len(content_tokens)\nuniform = np.ones(n) / n\nmi_approx = np.zeros((n, n))\nfor i in range(n):\n    for j in range(n):\n        if content_attn_norm[i, j] > 0:\n            mi_approx[i, j] = content_attn_norm[i, j] * np.log2(\n                content_attn_norm[i, j] / uniform[j])\n\nim2 = axes[1].imshow(mi_approx, cmap='Purples')\naxes[1].set_xticks(range(n))\naxes[1].set_yticks(range(n))\naxes[1].set_xticklabels(content_tokens, fontsize=13)\naxes[1].set_yticklabels(content_tokens, fontsize=13)\naxes[1].set_xlabel('Token j')\naxes[1].set_ylabel('Token i')\naxes[1].set_title('MI Approximation from Attention')\nplt.colorbar(im2, ax=axes[1], shrink=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"左图是注意力矩阵，每个格子表示一个token对另一个token的关注程度。\")\nprint(\"右图是从注意力权重近似出的互信息，颜色越深表示两个token之间的语义关联越强。\")\nprint()\nprint(\"比如，「猫」和「坐」之间互信息较高（主谓关系），「垫」和「子」也是（合成词）。\")\nprint(\"注意力机制并不是在直接计算互信息，但通过 Q·K 的点积，它隐式地捕捉了类似的依赖结构。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **《V-信息理论》** **V-Information Theory**\n",
    "https://arxiv.org/abs/2002.10689\n",
    "\n",
    "V-信息理论认为，信息的“量”不仅取决于数据的统计特性，还取决于有限计算资源下提取这些信息的难易程度。\n",
    "\n",
    "V-信息理论引入了**计算复杂度**的概念。即：如果提取信息需要更多的计算资源，那么这种表示的信息量在实际中是 **“更少”** 的。\n",
    "\n",
    "在人工智能中，嵌入和权重这些存储信息的表示中，哪些是“有效信息”，这可以根据有限计算资源下的提取方式的难易程度来进行衡量。比如：\n",
    "\n",
    "- 为什么模型蒸馏或LoRA等方法可保留更多有用信息？（优化了信息提取效率）\n",
    "- 为什么自注意力机制比其他架构更高效？（更好的“解码”可用信息）\n",
    "\n",
    "__原理__：\n",
    "通过一个模型集合$V$（原文叫做探测器），$V$里面包含了有限的观察者可用的提取方法，比如简单的线性模型、Transformer等神经网络模型来 **估计条件熵（CE）** 误差。或者说取最优模型的预测误差。\n",
    "\n",
    "目前只有启发意义，对比哪些建模可能更高效。有兴趣可以看看原论文。\n",
    "\n",
    "### 总结\n",
    "\n",
    "传统香农信息论为静态的统计数据。而V-信息论引入了主观因素（观察者的计算资源）的动态处理能力。\n"
   ],
   "id": "4461e6e708f2ae7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 热力学定律\n",
    "\n",
    "信息的处理需要消耗能量，比如擦除1比特的信息需要耗能。这就把计算与物理限制绑定在一起。\n",
    "\n",
    "__例子：__\n",
    "“改变原子中的电子数量，需要光子能量去撞击电子”\n",
    "\n",
    "\n",
    "信息的处理和提取过程受到某种类似热力学定律的限制。以至于 __我们能从训练语料中榨取的信息是有限的__。\n",
    "对于一个训练集，无论用什么方式（LoRA、蒸馏、SFT、Mamba、Transformer），都无法超越这个限制。它们只能优化提取效率，而不能**创造**新信息。\n"
   ],
   "id": "2da39e59df3e5ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 另一个角度去理解熵：压缩即“智能”？\n",
    "\n",
    "其实，香农熵公式定义了数据的最优压缩极限，即熵：\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} p(x_{i}) log_{2} p(x_{i})\n",
    "$$\n",
    "\n",
    "它是无损压缩的理论下界。例如很多压缩算法的最小编码长度不能低于其熵值。\n",
    "\n"
   ],
   "id": "55988fc635393476"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "压缩技术发展历史（1950～2025）：\n",
    "- Shannon-Fano Coding：高频符号分配较短编码、低频符号分配较长编码。\n",
    "- Huffman Coding：类似上面的，做了构建二叉树的优化，更加接近香农熵极限。\n",
    "- 字典编码法：通过滑动窗口查找重复模式，利用指针和长度替换冗余序列。后来发展成动态字典。（ZIP、gzip）\n",
    "- 算术编码：将整个信息编码成一个分数，理论上更接近香农熵极限，但计算过程复杂。\n",
    "- 离散余弦变换（DCT）：实现有损高度压缩，去除人眼不敏感的高频细节。（JPEG）\n",
    "- 基于Huffman编码的音频压缩：利用心理声学模型去除人耳不敏感的部分，实现有损压缩。（MP3）\n",
    "- 离散余弦变换（DCT）+时间维度：通过帧间预测结合DCT实现视频压缩。（MPEG、DVD、数字电视）\n",
    "- 神经网络压缩：\n",
    "    - Autoencoders\n",
    "    - RNN\n",
    "    - Transformer\n",
    "\n",
    "### 思考🤔通用压缩的极限？\n",
    "\n",
    "香农熵的极限边界是什么？结合语义信息，压缩不仅去除冗余，还能最大化保留任务相关内容吗？"
   ],
   "id": "6600a548a6578890"
  },
  {
   "cell_type": "markdown",
   "id": "2wm47efsuvk",
   "source": "上面提到香农熵定义了无损压缩的理论下界。这不只是理论——我们可以用代码验证。下面对不同熵的文本做压缩，看看实际压缩率和熵之间的关系。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n8qa09sny7o",
   "source": "import zlib\nimport math\nimport numpy as np\n\ndef entropy_per_char(text):\n    \"\"\"Compute per-character entropy in bits\"\"\"\n    freq = {}\n    for ch in text:\n        freq[ch] = freq.get(ch, 0) + 1\n    n = len(text)\n    return -sum((c/n) * math.log2(c/n) for c in freq.values())\n\ntexts = {\n    'Repeated':  'aaaa' * 250,\n    'English':   'the cat sat on the mat and the dog lay on the rug ' * 20,\n    'Chinese':   '猫坐在垫子上看着窗外的风景这是一个关于信息论的教程' * 40,\n    'Random':    ''.join(chr(np.random.randint(65, 91)) for _ in range(1000)),\n}\n\nfor name, text in texts.items():\n    raw = text.encode('utf-8')\n    compressed = zlib.compress(raw, 9)\n    ratio = len(compressed) / len(raw)\n    h = entropy_per_char(text)\n    print(f\"{name:>10}: {len(raw):>5}B -> {len(compressed):>5}B  \"\n          f\"(compression ratio {ratio:.1%}, entropy {h:.2f} bits/char)\")\n\nprint()\nprint(\"低熵的重复文本几乎可以被压缩到极小，而高熵的随机字符串基本压不动。\")\nprint(\"这就是香农熵的物理含义：它是数据中「真正的信息量」的度量。\")\nprint(\"从这个角度看，语言模型的训练其实就是在学习语言数据的压缩表示——预测得越准，压缩得越好。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}