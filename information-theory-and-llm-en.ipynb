{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information Theory** — _Claude Shannon_ (1948)\n",
    "\n",
    "1. **Information** is defined as a quantity that reduces uncertainty.\n",
    "\n",
    "2. The **information content** of an event is related to its **probability**. The less likely an event, the more information it carries when it occurs. $I(x) = -\\log_{2}(p(x))$\n",
    "\n",
    "3. **Entropy**: The average information content (or uncertainty) of a random variable.\n",
    "    For a discrete random variable $X$ with probability distribution $p(x)$, entropy is defined as: $H(X) = - \\sum p(x) \\log_{2}(p(x))$. Higher entropy means greater randomness (or uncertainty).\n",
    "\n",
    "\n",
    "Modern telegraph communication, ZIP file compression, JPEG image compression, cryptography, and more all stem from Shannon's information theory. The theory focuses on the statistical properties of data, without considering the semantics of the information or the computational cost of extracting it. For example,\n",
    "\n",
    "\n",
    "- A SHA-256 encrypted key\n",
    "- A national ID number\n",
    "\n",
    "Shannon entropy was later extended to quantum information: qubits and quantum entropy.\n",
    "\n",
    "An important point: from Shannon entropy's perspective, both examples above carry the same amount of information. But extracting their respective semantic information requires vastly different computational resources."
   ],
   "id": "b07213a1f523"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range for x values (non-negative values up to 1)\n",
    "x = np.linspace(0.001, 1, 100)\n",
    "\n",
    "# Define the function graph\n",
    "y = np.log2(x)\n",
    "\n",
    "# Plot the graph of y vs x\n",
    "plt.plot(x, y, label='log graph')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(f\"{np.log10(0.4)} — log base 10 of 0.4\")\n",
    "print(f\"{np.log(0.4)} — natural log (base e) of 0.4\")\n",
    "print(f\"{np.log2(0.4)} — log base 2 of 0.4\")"
   ],
   "id": "d617bc59e7a4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log graph above shows the behavior of $\\log_2(x)$ over the interval $(0, 1]$. Next, let's use code to build intuition for two things:\n",
    "\n",
    "1. **Binary entropy function**: When a random variable has only two outcomes (like a coin flip), how does its entropy change with probability?\n",
    "2. **Information content intuition**: How much information do events of different probabilities actually carry?"
   ],
   "id": "f657deafd783"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binary entropy function H(p) for a two-outcome random variable\n",
    "p = np.linspace(0.001, 0.999, 500)\n",
    "H = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: binary entropy curve\n",
    "axes[0].plot(p, H, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('p')\n",
    "axes[0].set_ylabel('H(p) bits')\n",
    "axes[0].set_title('Binary Entropy Function')\n",
    "axes[0].axvline(x=0.5, color='r', linestyle='--', alpha=0.5, label='p=0.5')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: information content of different events\n",
    "events = {\n",
    "    'Coin flip\\n(heads)': 0.5,\n",
    "    'Dice roll\\n(one face)': 1/6,\n",
    "    'Common\\nEnglish word': 0.04,\n",
    "    'Rare\\nword': 0.0001,\n",
    "    'Lottery': 1e-7,\n",
    "}\n",
    "names = list(events.keys())\n",
    "probs = list(events.values())\n",
    "info_content = [-np.log2(p_val) for p_val in probs]\n",
    "\n",
    "colors = ['#4CAF50', '#2196F3', '#FF9800', '#F44336', '#9C27B0']\n",
    "bars = axes[1].bar(names, info_content, color=colors)\n",
    "axes[1].set_ylabel('I(x) bits')\n",
    "axes[1].set_title('Information Content of Events')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val, prob in zip(bars, info_content, probs):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "                 f'{val:.1f} bits\\n(p={prob:.4g})', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# The key intuition: rarer events carry more information\n",
    "print(f\"Fair coin (heads):  I = -log2(0.5)  = {-np.log2(0.5):.1f} bits\")\n",
    "print(f\"Dice (one face):    I = -log2(1/6)  = {-np.log2(1/6):.2f} bits\")\n",
    "print(f\"Winning lottery:    I = -log2(1e-7) = {-np.log2(1e-7):.1f} bits\")\n",
    "print()\n",
    "print(\"The left plot shows entropy is maximized at p=0.5 — maximum uncertainty means maximum information.\")\n",
    "print(\"The right plot is intuitive: the rarer the event, the more information it carries when it happens.\")"
   ],
   "id": "a0cac7a039de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "For two discrete random variables $X$ and $Y$ (taking values $x_{i}$ and $y_{i}$), mutual information $I(X;Y)$ is defined as:\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log_2 \\left( \\frac{p(x, y)}{p(x)p(y)} \\right)\n",
    "$$\n",
    "\n",
    "For continuous random variables, the summation becomes an integral:\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\iint p(x, y) \\log_2 \\left( \\frac{p(x, y)}{p(x)p(y)} \\right) \\, dx \\, dy\n",
    "$$\n",
    "\n",
    "### Equivalent Forms of Mutual Information\n",
    "\n",
    "Mutual information can be expressed using entropy and conditional entropy, with several equivalent definitions:\n",
    "\n",
    "#### 1. Entropy-based definition:\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "\n",
    "- $H(X)$: Entropy (uncertainty) of $X$.\n",
    "- $H(Y)$: Entropy of $Y$.\n",
    "- $H(X,Y)$: Joint entropy of $X$ and $Y$.\n",
    "- **Meaning**: Mutual information is the sum of individual entropies minus the joint entropy, representing the redundant information between the two variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Conditional entropy-based definition:\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "- $H(X|Y)$: Conditional entropy of $X$ given $Y$.\n",
    "- **Meaning**: Mutual information is the reduction in uncertainty about $X$ after observing $Y$.\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "\n",
    "- **Non-negativity**:\n",
    "  - $I(X;Y) \\geq 0$: Mutual information is always non-negative, because dependence cannot increase uncertainty.\n",
    "  - When $X$ and $Y$ are independent, $I(X;Y) = 0$.\n",
    "\n",
    "\n",
    "- **Upper bound**:\n",
    "  - $I(X;Y) \\leq H(X)$ and $I(X;Y) \\leq H(Y)$: Mutual information cannot exceed the entropy of either variable.\n",
    "  - If $X = Y$ (perfectly correlated), then $I(X;Y) = H(X) = H(Y)$.\n",
    "\n",
    "\n",
    "- **Symmetry**:\n",
    "  - $I(X;Y) = I(Y;X)$: Mutual information does not depend on the order of variables.\n",
    "\n",
    "\n",
    "- **Units**:\n",
    "  - When computed with $\\log_2$, the unit is bits;\n",
    "  - When using natural log ($\\ln$), the unit is nats."
   ],
   "id": "335e4592ba62"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we listed the mathematical definition of mutual information and its equivalent forms. Let's compute a concrete example and draw the classic Venn diagram showing the relationships between $H(X)$, $H(Y)$, $H(X,Y)$, $H(X|Y)$, and $I(X;Y)$."
   ],
   "id": "2bde6c85be93"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Weather(X) vs Umbrella(Y)\n",
    "# X: 0=sunny, 1=rainy | Y: 0=no umbrella, 1=umbrella\n",
    "p_xy = np.array([\n",
    "    [0.45, 0.05],  # sunny: no umbrella=0.45, umbrella=0.05\n",
    "    [0.05, 0.45],  # rainy: no umbrella=0.05, umbrella=0.45\n",
    "])\n",
    "\n",
    "p_x = p_xy.sum(axis=1)\n",
    "p_y = p_xy.sum(axis=0)\n",
    "\n",
    "H_X = -np.sum(p_x * np.log2(p_x))\n",
    "H_Y = -np.sum(p_y * np.log2(p_y))\n",
    "H_XY = -np.sum(p_xy * np.log2(p_xy + 1e-10))\n",
    "H_X_given_Y = H_XY - H_Y\n",
    "H_Y_given_X = H_XY - H_X\n",
    "I_XY = H_X + H_Y - H_XY\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: joint distribution heatmap\n",
    "im = axes[0].imshow(p_xy, cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['No umbrella (Y=0)', 'Umbrella (Y=1)'], fontsize=11)\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_yticklabels(['Sunny (X=0)', 'Rainy (X=1)'], fontsize=11)\n",
    "axes[0].set_title('Joint Distribution P(X, Y)')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, f'{p_xy[i,j]:.2f}', ha='center', va='center',\n",
    "                     fontsize=18, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# Right: Venn diagram of entropy relationships\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-2, 3.5)\n",
    "ax.set_ylim(-1.8, 1.8)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Entropy Venn Diagram')\n",
    "\n",
    "circle_x = plt.Circle((0.5, 0), 1.2, fill=False, edgecolor='#2196F3', linewidth=2.5)\n",
    "circle_y = plt.Circle((1.5, 0), 1.2, fill=False, edgecolor='#F44336', linewidth=2.5)\n",
    "ax.add_patch(circle_x)\n",
    "ax.add_patch(circle_y)\n",
    "\n",
    "ax.text(-0.3, 0, f'H(X|Y)\\n{H_X_given_Y:.3f}', ha='center', va='center',\n",
    "        fontsize=11, color='#1565C0', fontweight='bold')\n",
    "ax.text(1.0, 0, f'I(X;Y)\\n{I_XY:.3f}', ha='center', va='center',\n",
    "        fontsize=12, color='#6A1B9A', fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#E1BEE7', alpha=0.7))\n",
    "ax.text(2.3, 0, f'H(Y|X)\\n{H_Y_given_X:.3f}', ha='center', va='center',\n",
    "        fontsize=11, color='#C62828', fontweight='bold')\n",
    "ax.text(0.5, -1.6, f'H(X) = {H_X:.3f}', ha='center', fontsize=11, color='#2196F3')\n",
    "ax.text(1.5, -1.6, f'H(Y) = {H_Y:.3f}', ha='center', fontsize=11, color='#F44336')\n",
    "ax.text(1.0, 1.6, f'H(X,Y) = {H_XY:.3f}', ha='center', fontsize=11, color='#333')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In this example, X is weather (sunny/rainy) and Y is whether someone carries an umbrella.\")\n",
    "print(f\"Weather uncertainty    H(X)   = {H_X:.4f} bits\")\n",
    "print(f\"Umbrella uncertainty   H(Y)   = {H_Y:.4f} bits\")\n",
    "print(f\"Joint uncertainty      H(X,Y) = {H_XY:.4f} bits\")\n",
    "print(f\"Mutual information     I(X;Y) = {I_XY:.4f} bits\")\n",
    "print()\n",
    "print(f\"If I already know someone is carrying an umbrella, my uncertainty about the weather\")\n",
    "print(f\"drops from {H_X:.3f} to {H_X_given_Y:.3f} bits.\")\n",
    "print(\"That's the intuition behind mutual information — how much knowing one variable reduces uncertainty about the other.\")"
   ],
   "id": "a6aed57eb083"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Machine Learning\n",
    "\n",
    "- Feature selection: Choose features with the highest mutual information with the target variable.\n",
    "- Information bottleneck: Optimize models to compress input information while preserving mutual information with the output.\n",
    "- Language modeling: Measure semantic dependencies between words.\n",
    "\n",
    "\n",
    "1. **Relationships between tokens**:\n",
    "\n",
    "  - In a Transformer, the input is a sequence of tokens (e.g., words or subwords). Through self-attention, each token updates its representation based on other tokens in the context. Attention scores reflect the importance and dependency between tokens.\n",
    "\n",
    "  - From an information-theoretic perspective, the mutual information $I(X_i; X_j)$ between two tokens can be understood as: how much observing $X_j$ (one token's representation) reduces uncertainty about $X_i$ (another token's representation).\n",
    "\n",
    "  - For example, in the sentence \"The cat sat on the mat\", \"cat\" and \"sat\" are linked through the attention mechanism. If \"sat\" provides contextual information about \"cat\" (e.g., subject-verb relationship), their mutual information is greater than zero.\n",
    "\n",
    "2. **How attention reflects mutual information**:\n",
    "\n",
    "  - Self-attention computes each token's weighted contribution to other tokens. The weights (post-softmax attention scores) can be seen as an approximation of conditional probability: $P(X_i \\mid X_j)$. Computing mutual information involves the ratio of joint distribution $p(X_i, X_j)$ to the product of marginal distributions $p(X_i)p(X_j)$, and the attention mechanism implicitly captures this dependency.\n",
    "\n",
    "  - Specifically, the attention mechanism quantifies the correlation between tokens through the dot product of Keys, Queries, and Values — this is similar in spirit to how mutual information measures shared information between variables.\n",
    "\n",
    "3. **Sampling and modeling**:\n",
    "\n",
    "  - During training or inference, token sequences are progressively refined through multi-layer attention mechanisms. Each layer's output can be seen as enhanced extraction of mutual information between input tokens. For example, after multiple Transformer layers, the representations of \"cat\" and \"mat\" may contain more shared information about their semantic relationship.\n",
    "\n",
    "\n",
    "_The attention mechanism in Transformers does not directly compute mutual information. Instead, it captures dependencies indirectly through parameterization (weight matrices) and training objectives (e.g., cross-entropy loss for language modeling). It is an approximation or practical implementation of mutual information, not a strict mathematical computation._\n",
    "\n",
    "- The Transformer training objective is typically to minimize cross-entropy loss, which is equivalent to minimizing the KL divergence between the predicted distribution and the true distribution.\n",
    "- $H(Y \\mid X)$ (conditional entropy) represents the uncertainty when the language model predicts the next token, while mutual information $I(X;Y)$ reflects how much context $X$ helps predict $Y$.\n",
    "- Information theory can inspire more efficient generation algorithms, e.g., optimizing coherence by maximizing $I(X_{t-1}; X_t)$.\n",
    "\n",
    "- **Knowledge Distillation**:\n",
    "\n",
    "  Mutual information can measure the amount of shared knowledge between teacher and student models. The distillation process tries to preserve the key parts of $I(X;Y)$.\n",
    "\n",
    "\n",
    "- **LoRA and Fine-tuning**:\n",
    "\n",
    "   LoRA (Low-Rank Adaptation) adjusts weights through low-rank updates, which may affect the mutual information distribution in representations. Information theory can help explain why LoRA is more efficient than full-parameter fine-tuning (e.g., preserving task-relevant mutual information)."
   ],
   "id": "765d0f9846e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss is the core objective function for LLM training. It has a clean relationship with KL divergence:\n",
    "\n",
    "$$H(P, Q) = H(P) + D_{KL}(P \\| Q)$$\n",
    "\n",
    "In other words, cross-entropy = true entropy + KL divergence. Minimizing cross-entropy during training is really about minimizing KL divergence — pushing the model's predicted distribution as close as possible to the true distribution. Let's verify this relationship with a character frequency example."
   ],
   "id": "9865d58968c6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated English letter frequency distribution\n",
    "chars = ['e', 't', 'a', 'o', 'i', 'n', 's', 'h', 'r', 'other']\n",
    "p_true = np.array([0.127, 0.091, 0.082, 0.075, 0.070, 0.067, 0.063, 0.061, 0.060, 0.304])\n",
    "\n",
    "# Three \"model\" predictions with different quality\n",
    "q_good    = np.array([0.125, 0.089, 0.080, 0.074, 0.069, 0.066, 0.064, 0.062, 0.061, 0.310])\n",
    "q_bad     = np.array([0.050, 0.050, 0.050, 0.100, 0.100, 0.100, 0.100, 0.050, 0.050, 0.350])\n",
    "q_uniform = np.ones(len(chars)) / len(chars)\n",
    "\n",
    "# Sanity check: all distributions must sum to 1\n",
    "for name, dist in [('p_true', p_true), ('q_good', q_good), ('q_bad', q_bad)]:\n",
    "    assert abs(dist.sum() - 1.0) < 1e-9, f\"{name} sums to {dist.sum()}, not 1.0\"\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log2(q + 1e-10))\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log2((p + 1e-10) / (q + 1e-10)))\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p + 1e-10))\n",
    "\n",
    "H_p = entropy(p_true)\n",
    "distributions = {'Good model': q_good, 'Bad model': q_bad, 'Uniform': q_uniform}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: compare distributions (show top 9 characters)\n",
    "x_pos = np.arange(len(chars) - 1)\n",
    "width = 0.2\n",
    "axes[0].bar(x_pos - 1.5*width, p_true[:-1], width, label='True P', color='#333', alpha=0.8)\n",
    "axes[0].bar(x_pos - 0.5*width, q_good[:-1], width, label='Good model', color='#4CAF50', alpha=0.7)\n",
    "axes[0].bar(x_pos + 0.5*width, q_bad[:-1], width, label='Bad model', color='#FF9800', alpha=0.7)\n",
    "axes[0].bar(x_pos + 1.5*width, q_uniform[:-1], width, label='Uniform', color='#F44336', alpha=0.7)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(chars[:-1], fontsize=11)\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_title('True vs Predicted Distributions')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: cross-entropy and KL divergence comparison\n",
    "labels = list(distributions.keys())\n",
    "ce_vals = [cross_entropy(p_true, q) for q in distributions.values()]\n",
    "kl_vals = [kl_divergence(p_true, q) for q in distributions.values()]\n",
    "\n",
    "x_pos2 = np.arange(len(labels))\n",
    "bars1 = axes[1].bar(x_pos2 - 0.2, ce_vals, 0.35, label='Cross-Entropy H(P,Q)', color='#2196F3', alpha=0.8)\n",
    "bars2 = axes[1].bar(x_pos2 + 0.2, kl_vals, 0.35, label='KL Divergence', color='#F44336', alpha=0.8)\n",
    "axes[1].axhline(y=H_p, color='green', linestyle='--', linewidth=2, label=f'True Entropy H(P)={H_p:.3f}')\n",
    "axes[1].set_xticks(x_pos2)\n",
    "axes[1].set_xticklabels(labels, fontsize=11)\n",
    "axes[1].set_ylabel('Bits')\n",
    "axes[1].set_title('H(P,Q) = H(P) + D_KL(P||Q)')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars1, ce_vals):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                 f'{val:.3f}', ha='center', fontsize=9)\n",
    "for bar, val in zip(bars2, kl_vals):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                 f'{val:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True distribution entropy H(P) = {H_p:.4f} bits\\n\")\n",
    "for name, q in distributions.items():\n",
    "    ce = cross_entropy(p_true, q)\n",
    "    kl = kl_divergence(p_true, q)\n",
    "    print(f\"  {name}: cross-entropy = {ce:.4f}, KL divergence = {kl:.4f}\")\n",
    "    print(f\"    Verify: H(P) + D_KL = {H_p:.4f} + {kl:.4f} = {H_p + kl:.4f}\")\n",
    "print()\n",
    "print(\"The closer a model's predictions are to the true distribution, the smaller the KL divergence and cross-entropy.\")\n",
    "print(\"This is why LLMs use cross-entropy as their loss function — it directly measures the gap between the model and real language.\")"
   ],
   "id": "a2fb5d118bde"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we mentioned that the self-attention mechanism quantifies dependencies between tokens via the $Q \\cdot K^T$ dot product, which is similar in spirit to how mutual information measures shared information between variables. Let's visualize this — using the sentence \"The cat sat on the mat\", we'll look at the attention matrix and approximate the mutual information between tokens."
   ],
   "id": "5c5cf6063f1a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try loading a pretrained model; fall back to simulated data if unavailable\n",
    "use_model = False\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "    text = \"The cat sat on the mat\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Take the last layer's attention, averaged across all heads\n",
    "    attention = outputs.attentions[-1].squeeze(0).mean(dim=0).numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    use_model = True\n",
    "    print(\"Loaded bert-base-uncased model\\n\")\n",
    "except Exception:\n",
    "    print(\"transformers/torch not installed, using simulated data\")\n",
    "    print(\"To use a real model: pip install transformers torch\\n\")\n",
    "\n",
    "    tokens = ['[CLS]', 'the', 'cat', 'sat', 'on', 'the', 'mat', '[SEP]']\n",
    "    # Simulated attention pattern based on linguistic intuition\n",
    "    attention = np.array([\n",
    "        [0.30, 0.10, 0.12, 0.10, 0.08, 0.10, 0.12, 0.08],  # CLS -> global\n",
    "        [0.05, 0.15, 0.30, 0.15, 0.10, 0.10, 0.10, 0.05],  # the -> cat (determiner)\n",
    "        [0.05, 0.10, 0.20, 0.35, 0.10, 0.05, 0.10, 0.05],  # cat -> sat (subject-verb)\n",
    "        [0.05, 0.05, 0.30, 0.15, 0.20, 0.05, 0.15, 0.05],  # sat -> cat, on\n",
    "        [0.05, 0.05, 0.05, 0.25, 0.15, 0.15, 0.25, 0.05],  # on -> sat, mat\n",
    "        [0.05, 0.10, 0.05, 0.05, 0.10, 0.15, 0.40, 0.10],  # the -> mat (determiner)\n",
    "        [0.05, 0.05, 0.10, 0.20, 0.25, 0.20, 0.10, 0.05],  # mat -> on, sat\n",
    "        [0.12, 0.12, 0.13, 0.13, 0.12, 0.12, 0.13, 0.13],  # SEP -> global\n",
    "    ])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "# Left: attention heatmap\n",
    "im = axes[0].imshow(attention, cmap='Reds', vmin=0)\n",
    "axes[0].set_xticks(range(len(tokens)))\n",
    "axes[0].set_yticks(range(len(tokens)))\n",
    "axes[0].set_xticklabels(tokens, fontsize=12)\n",
    "axes[0].set_yticklabels(tokens, fontsize=12)\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "axes[0].set_title('Attention Matrix')\n",
    "plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# Right: approximate mutual information from attention weights\n",
    "# Strip [CLS] and [SEP], focus on content tokens\n",
    "content_tokens = tokens[1:-1]\n",
    "content_attn = attention[1:-1, 1:-1]\n",
    "content_attn_norm = content_attn / content_attn.sum(axis=1, keepdims=True)\n",
    "\n",
    "n = len(content_tokens)\n",
    "uniform = np.ones(n) / n\n",
    "mi_approx = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if content_attn_norm[i, j] > 0:\n",
    "            mi_approx[i, j] = content_attn_norm[i, j] * np.log2(\n",
    "                content_attn_norm[i, j] / uniform[j])\n",
    "\n",
    "im2 = axes[1].imshow(mi_approx, cmap='Purples')\n",
    "axes[1].set_xticks(range(n))\n",
    "axes[1].set_yticks(range(n))\n",
    "axes[1].set_xticklabels(content_tokens, fontsize=13)\n",
    "axes[1].set_yticklabels(content_tokens, fontsize=13)\n",
    "axes[1].set_xlabel('Token j')\n",
    "axes[1].set_ylabel('Token i')\n",
    "axes[1].set_title('MI Approximation from Attention')\n",
    "plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The left plot is the attention matrix — each cell shows how much one token attends to another.\")\n",
    "print(\"The right plot approximates mutual information from the attention weights — darker means stronger semantic association.\")\n",
    "print()\n",
    "print(\"For example, 'cat' and 'sat' have high mutual information (subject-verb), as do 'on' and 'mat' (prepositional).\")\n",
    "print(\"The attention mechanism doesn't directly compute mutual information, but through the Q*K dot product,\")\n",
    "print(\"it implicitly captures a similar dependency structure.\")"
   ],
   "id": "54478e5ed29c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **V-Information Theory**\n",
    "https://arxiv.org/abs/2002.10689\n",
    "\n",
    "V-information theory argues that the \"amount\" of information depends not only on the statistical properties of the data, but also on how easy it is to extract that information given limited computational resources.\n",
    "\n",
    "V-information theory introduces the concept of **computational complexity**. That is: if extracting information requires more computational resources, then the information content of that representation is effectively **\"less\"** in practice.\n",
    "\n",
    "In AI, among the information stored in embeddings and weights, which parts are \"useful information\"? This can be measured by how easy or hard it is to extract them with limited computational resources. For example:\n",
    "\n",
    "- Why can methods like model distillation or LoRA preserve more useful information? (They optimize information extraction efficiency)\n",
    "- Why is the self-attention mechanism more efficient than other architectures? (Better at \"decoding\" available information)\n",
    "\n",
    "__Principle__:\n",
    "Through a model family $V$ (called \"probes\" in the paper), which contains a finite set of extraction methods available to the observer — simple linear models, Transformer-based neural networks, etc. — one **estimates conditional entropy (CE)** error. In other words, take the prediction error of the best model.\n",
    "\n",
    "Currently this has mainly heuristic value, useful for comparing which modeling approaches might be more efficient. See the original paper for details.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Traditional Shannon information theory deals with static statistical data. V-information theory introduces a subjective factor — the observer's computational resources — creating a dynamic framework for information processing."
   ],
   "id": "229110747fe7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thermodynamic Laws\n",
    "\n",
    "Processing information requires energy — for instance, erasing 1 bit of information requires a minimum energy expenditure. This binds computation to physical limits.\n",
    "\n",
    "__Example:__\n",
    "\"Changing the number of electrons in an atom requires photon energy to knock out electrons\"\n",
    "\n",
    "\n",
    "The process of extracting and processing information is subject to constraints analogous to the laws of thermodynamics. Consequently, __the information we can extract from training data is fundamentally bounded__.\n",
    "For a given training set, no matter what method is used (LoRA, distillation, SFT, Mamba, Transformer), this limit cannot be exceeded. These methods can only optimize extraction efficiency — they cannot **create** new information."
   ],
   "id": "a370a2469667"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Way to Understand Entropy: Is Compression \"Intelligence\"?\n",
    "\n",
    "Shannon's entropy formula defines the optimal compression limit for data — the entropy:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} p(x_{i}) \\log_{2} p(x_{i})\n",
    "$$\n",
    "\n",
    "It is the theoretical lower bound for lossless compression. For example, the minimum encoding length of many compression algorithms cannot go below the entropy value."
   ],
   "id": "6acbde8204b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief history of compression techniques (1950-2025):\n",
    "- Shannon-Fano Coding: Assigns shorter codes to high-frequency symbols and longer codes to low-frequency symbols.\n",
    "- Huffman Coding: Similar approach with binary tree optimization, getting closer to the Shannon entropy limit.\n",
    "- Dictionary encoding: Finds repeated patterns through sliding windows, replacing redundant sequences with pointers and lengths. Later evolved into dynamic dictionaries. (ZIP, gzip)\n",
    "- Arithmetic coding: Encodes the entire message as a single fraction, theoretically closer to the Shannon entropy limit but computationally complex.\n",
    "- Discrete Cosine Transform (DCT): Achieves high lossy compression by removing high-frequency details imperceptible to the human eye. (JPEG)\n",
    "- Huffman-based audio compression: Uses psychoacoustic models to remove parts inaudible to the human ear for lossy compression. (MP3)\n",
    "- DCT + temporal dimension: Combines inter-frame prediction with DCT for video compression. (MPEG, DVD, digital TV)\n",
    "- Neural network compression:\n",
    "    - Autoencoders\n",
    "    - RNN\n",
    "    - Transformer\n",
    "\n",
    "### The Limits of Universal Compression?\n",
    "\n",
    "What are the boundaries of Shannon entropy's limits? By incorporating semantic information, can compression not only remove redundancy but also maximize preservation of task-relevant content?"
   ],
   "id": "2baa91ba25ba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we discussed how Shannon entropy defines the theoretical lower bound for lossless compression. This isn't just theory — we can verify it with code. Let's compress texts with different entropy levels and see how the actual compression ratio relates to entropy."
   ],
   "id": "7bd811543ea8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import zlib\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def entropy_per_char(text):\n",
    "    \"\"\"Compute per-character entropy in bits\"\"\"\n",
    "    freq = {}\n",
    "    for ch in text:\n",
    "        freq[ch] = freq.get(ch, 0) + 1\n",
    "    n = len(text)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in freq.values())\n",
    "\n",
    "texts = {\n",
    "    'Repeated':  'aaaa' * 250,\n",
    "    'English':   'the cat sat on the mat and the dog lay on the rug ' * 20,\n",
    "    'Mixed':     'Information theory connects probability to communication ' * 20,\n",
    "    'Random':    ''.join(chr(np.random.randint(65, 91)) for _ in range(1000)),\n",
    "}\n",
    "\n",
    "for name, text in texts.items():\n",
    "    raw = text.encode('utf-8')\n",
    "    compressed = zlib.compress(raw, 9)\n",
    "    ratio = len(compressed) / len(raw)\n",
    "    h = entropy_per_char(text)\n",
    "    print(f\"{name:>10}: {len(raw):>5}B -> {len(compressed):>5}B  \"\n",
    "          f\"(compression ratio {ratio:.1%}, entropy {h:.2f} bits/char)\")\n",
    "\n",
    "print()\n",
    "print(\"Low-entropy repeated text can be compressed to almost nothing, while high-entropy random strings barely compress at all.\")\n",
    "print(\"This is the physical meaning of Shannon entropy: it measures the 'true information content' in the data.\")\n",
    "print(\"From this perspective, training a language model is really about learning a compressed representation of language —\")\n",
    "print(\"better predictions mean better compression.\")"
   ],
   "id": "506beeed12b9"
  }
 ]
}